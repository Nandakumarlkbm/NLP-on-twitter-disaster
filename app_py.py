# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E70xW58W2O0cfj49pSeSr3gC7RbWFJQe
"""

import streamlit as st
import pandas as pd
import numpy as np
import pickle
import nltk
import re
import string 
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
import contractions
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.stem import WordNetLemmatizer

st.title("IDENTIFICATION OF TWEETS RELATED TO DISASTER AND THOSE NOT RELATED TO DISASTER")

st.write("You can enter your tweet below and the model trained on labeled tweet data provided by Kaggle can predict with above 80% accuracy whether the tweet is related to disaster or not")

st.image(image="https://twitter.com/Twitter/photo")

tweet = st.text_input(label="Enter the tweet you want to identify below:",value="", max_chars=None,placeholder="Enter your tweet here")





def classifiertwitter(tweet):
    print("Analysing tweet...")
    def contractionfun(text):
      expanded_words=[]
      for word in text.split():
        expanded_words.append(contractions.fix(word))
      return '  '.join(expanded_words)
    def url_remover(text):
      url_patterns = re.sub(r'''(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'".,<>?«»“”‘’]))''', " ", text)
      return url_patterns
    def preprocessing(sentence):  
      wl = WordNetLemmatizer()
      tweets = []
      sentence = sentence.lower() # converting the words to lower case
      sentence =  url_remover(sentence) # removing the url from the sentence
      sentence = re.sub(r'@w+',  '', sentence).strip() # removing the words starts with "@"
      sentence = contractionfun(sentence)
      sentence = re.sub("[^a-zA-Z0-9 ']", " ", sentence) # removing symbols
      sentence = sentence.replace("'","")
      sentence = sentence.encode('ascii', 'ignore').decode('utf8').strip()
      sentence = sentence.split()
      sentence1 = [wl.lemmatize(word) for word in sentence if word not in set(stopwords.words("english"))] #lemmatization and stopwords removal from tweets
      sentence1 = " ".join(sentence1)
      tweets.append(sentence1)
      return tweets

    x=preprocessing(tweet)

    with open('tokenizer.pickle', 'rb') as handle:
      tokenizer=pickle.load(handle)

    enc = tokenizer.texts_to_sequences([x])

    X_pad_tokens = pad_sequences(enc, maxlen=27, padding='post')

    model = load_model('https://github.com/Nandakumarlkbm/NLP-on-twitter-disaster/blob/main/best_model_crawlembeddingconv1d.h5')

    y_pred = model.predict(X_pad_tokens)

    if y_pred>=0.5:
      st.write("Tweet is related to disaster")
    else:
      st.write("Tweet is not related to disaster")

if st.button("Predict"):
       classifiertwitter(tweet)
